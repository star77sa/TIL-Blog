{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ideal-dragon",
   "metadata": {},
   "source": [
    "# CS231n_CNN for Visual Recognition\n",
    "> Stanford University CS231n ~ Lecture 7. CNN\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Pytorch]\n",
    "- image: images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-participant",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-bidding",
   "metadata": {},
   "source": [
    "- http://cs231n.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-interference",
   "metadata": {},
   "source": [
    "---\n",
    "# Image Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ee35f-0717-40dc-894b-fde58e6fed37",
   "metadata": {},
   "source": [
    "- **Image Classification:** We are given a **Training Set** of labeled images, asked to predict labels on **Test Set.** Common to report the **Accuracy** of predictions(fraction of correctly predicted images)\n",
    "\n",
    "- We introduced the **k-Nearest Neighbor Classifier**, which predicts the labels based on nearest images in the training set\n",
    "\n",
    "- We saw that the choice of distance and the value of k are **hyperparameters** that are tuned using a **validation set**, or through **cross-validation** if the size of the data is small.\n",
    "\n",
    "- Once the best set of hyperparameters is chosen, the classifier is evaluated once on the test set, and reported as the performance of kNN on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addedb3-7b0c-4d2e-ae66-5f0651f19789",
   "metadata": {},
   "source": [
    "- Nearest Neighbor 분류기는 CIFAR-10 데이터셋에서 약 40% 정도의 정확도를 보이는 것을 확인하였다. 이 방법은 구현이 매우 간단하지만, 학습 데이터셋 전체를 메모리에 저장해야 하고, 새로운 테스트 이미지를 분류하고 평가할 때 계산량이 매우 많다.\n",
    "\n",
    "- 단순히 픽셀 값들의 L1이나 L2 거리는 이미지의 클래스보다 배경이나 이미지의 전체적인 색깔 분포 등에 더 큰 영향을 받기 때문에 이미지 분류 문제에 있어서 충분하지 못하다는 점을 보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fda52a-1038-4b09-b774-0b2fa02fd758",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1345d-3cb9-46a5-a5d5-99772847f533",
   "metadata": {},
   "source": [
    "- We defined a **score function** from image pixels to class scores (in this section, a linear function that depends on weights **W** and biases **b**).\n",
    "\n",
    "- Unlike kNN classifier, the advantage of this **parametric approach** is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with **W**, not an exhaustive comparison to every single training example.\n",
    "\n",
    "- We introduced the **bias trick**, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix.\n",
    "하나의 매개변수 행렬만 추적해야 하는 편의를 위해 편향 벡터를 가중치 행렬로 접을 수 있는 편향 트릭을 도입했습니다 .\n",
    "\n",
    "- We defined a **loss function** (we introduced two commonly used losses for linear classifiers: the **SVM** and the **Softmax**) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b014d4-5b29-4818-9c1c-70a28eb57c83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e373643-a525-4ec7-9a36-67ad34783a9e",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf32a3-d8ea-469a-9c82-03d9f9626ba8",
   "metadata": {},
   "source": [
    "- We developed the intuition of the loss function as a **high-dimensional optimization landscape** in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped.\n",
    "\n",
    "- We motivated the idea of optimizing the loss function with **iterative refinement**, where we start with a random set of weights and refine them step by step until the loss is minimized.\n",
    "\n",
    "- We saw that the **gradient** of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of h used in computing the numerical gradient).\n",
    "\n",
    "- We saw that the parameter update requires a tricky setting of the **step size** (or the **learning rate**) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections.\n",
    "\n",
    "- We discussed the tradeoffs between computing the **numerical** and **analytic** gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a **gradient check**, in which its implementation is compared to the numerical gradient.\n",
    "\n",
    "- We introduced the **Gradient Descent** algorithm which iteratively computes the gradient and performs a parameter update in loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17e151-331e-48ed-be8c-2419d1af9235",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c5d41-2c75-4350-8595-a4477fb6a79d",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd54cf0-8ffd-4d60-b633-aff8d02ca8cf",
   "metadata": {},
   "source": [
    "- We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher.\n",
    "\n",
    "- We discussed the importance of **staged computation** for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5069b-9464-424e-a056-de2ab15483de",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Network - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709a149-7b03-4042-9865-f75422430fd1",
   "metadata": {},
   "source": [
    "- We introduced a very coarse model of a biological **neuron**\n",
    "\n",
    "- 실제 사용되는 몇몇 **활성화 함수** 에 대해 논의하였고, ReLU가 가장 일반적인 선택이다.\n",
    "    - 활성화 함수 쓰는 이유 : 데이터를 비선형으로 바꾸기 위해서. 선형이면 은닉층이 1개밖에 안나옴\n",
    "\n",
    "\n",
    "- We introduced **Neural Networks** where neurons are connected with **Fully-Connected layers** where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.\n",
    "\n",
    "- 우리는 layered architecture를 통해 활성화 함수의 기능 적용과 결합된 행렬 곱을 기반으로 신경망을 매우 효율적으로 평가할 수 있음을 보았다.\n",
    "\n",
    "- 우리는 Neural Networks가 **universal function approximators** 임을 보았지만, 우리는 또한 이 특성이 그들의 편재적인 사용과 거의 관련이 없다는 사실에 대해 논의하였다. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice.\n",
    "\n",
    "- 우리는 큰 network가 작은 network보다 항상 더 잘 작동하지만, 더 높은 model capacity는 더 강력한 정규화(높은 가중치 감쇠같은)로 적절히 해결되어야 하며, 그렇지 않으면 오버핏될 수 있다는 사실에 대해 논의하였다. 이후 섹션에서 더 많은 형태의 정규화(특히 dropout)를 볼 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db2688-eeeb-487d-8aae-2311b5054ad6",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Network - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed00a2e-c0d6-41b3-b5ed-5085e2bd2520",
   "metadata": {},
   "source": [
    "- 권장되는 전처리는 데이터의 중앙에 평균이 0이 되도록 하고 (zero centered), 스케일을 [-1, 1]로 정규화 하는 것 입니다.\n",
    "    - 올바른 전처리 방법 : 예를들어 평균차감 기법을 쓸 때 학습, 검증, 테스트를 위한 데이터를 먼저 나눈 후 학습 데이터를 대상으로 평균값을 구한 후에 평균차감 전처리를 모든 데이터군(학습, 검증, 테스트)에 적용하는 것이다.\n",
    "\n",
    "- ReLU를 사용하고 초기화는 $\\sqrt{2/n}$ 의 표준 편차를 갖는 정규 분포에서 가중치를 가져와 초기화합니다. 여기서 $n$은 뉴런에 대한 입력 수입니다. E.g. in numpy: `w = np.random.randn(n) * sqrt(2.0/n)`.\n",
    "\n",
    "- L2 regularization과 드랍아웃을 사용 (the inverted version)\n",
    "\n",
    "- Batch normalization 사용 (이걸쓰면 드랍아웃은 잘 안씀)\n",
    "\n",
    "- 실제로 수행할 수 있는 다양한 작업과 각 작업에 대한 가장 일반적인 손실 함수에 대해 논의했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc6577-66aa-44c2-8d0c-80ff625335db",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Network - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223287b-ae47-4df6-8fce-621bb018bb6e",
   "metadata": {},
   "source": [
    "신경망(neural network)를 훈련하기 위하여:\n",
    "\n",
    "- 코드를 짜는 중간중간에 작은 배치로 그라디언트를 체크하고, 뜻하지 않게 튀어나올 위험을 인지하고 있어야 한다.\n",
    "\n",
    "- 코드가 제대로 돌아가는지 확인하는 방법으로, 손실함수값의 초기값이 합리적인지 그리고 데이터의 일부분으로 100%의 훈련 정확도를 달성할 수 있는지 확인해야한다.\n",
    "\n",
    "- 훈련 동안, 손실함수와 train/validation 정확도를 계속 살펴보고, (이게 좀 더 멋져 보이면) 현재 파라미터 값 대비 업데이트 값 또한 살펴보라 (대충 ~ 1e-3 정도 되어야 한다). 만약 ConvNet을 다루고 있다면, 첫 층의 웨이트값도 살펴보라.\n",
    "\n",
    "- 업데이트 방법으로 추천하는 건 SGD+Nesterov Momentum 혹은 Adam이다.\n",
    "\n",
    "- 학습 속도를 훈련 동안 계속 하강시켜라. 예를 들면, 정해진 에폭 수 뒤에 (혹은 검증 정확도가 상승하다가 하강세로 꺾이면) 학습 속도를 반으로 깎아라.\n",
    "\n",
    "- Hyper parameter 검색은 grid search가 아닌 random search으로 수행하라. 처음에는 성긴 규모에서 탐색하다가 (넓은 hyper parameter 범위, 1-5 epoch 정도만 학습), 점점 촘촘하게 검색하라. (좁은 범위, 더 많은 에폭에서 학습)\n",
    "- 추가적인 개선을 위하여 모형 앙상블을 구축하라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2222907-8c31-4b3a-826a-50ec4a3ce8e6",
   "metadata": {},
   "source": [
    "---\n",
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad385e2-3a15-488d-8cdf-2c3cc1484156",
   "metadata": {},
   "source": [
    "- ConvNet 아키텍쳐는 여러 레이어를 통해 입력 이미지 볼륨을 출력 볼륨 (클래스 점수)으로 변환시켜 준다.\n",
    "\n",
    "- ConvNet은 몇 가지 종류의 레이어로 구성되어 있다. CONV/FC/RELU/POOL 레이어가 현재 가장 많이 쓰인다.\n",
    "\n",
    "- 각 레이어는 3차원의 입력 볼륨을 미분 가능한 함수를 통해 3차원 출력 볼륨으로 변환시킨다.\n",
    "\n",
    "- parameter가 있는 레이어도 있고 그렇지 않은 레이어도 있다 (FC/CONV는 parameter를 갖고 있고, RELU/POOL 등은 parameter가 없음).\n",
    "\n",
    "- hyperparameter가 있는 레이어도 있고 그렇지 않은 레이어도 있다 (CONV/FC/POOL 레이어는 hyperparameter를 가지며 ReLU는 가지지 않음).\n",
    "\n",
    "- stride, zero-padding ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a1737-22de-4335-9059-d1e8557e47c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
