{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ideal-dragon",
   "metadata": {},
   "source": [
    "# CS231n_CNN for Visual Recognition\n",
    "> Stanford University CS231n\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Pytorch]\n",
    "- image: images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-participant",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-bidding",
   "metadata": {},
   "source": [
    "- http://cs231n.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-interference",
   "metadata": {},
   "source": [
    "---\n",
    "# Image Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ee35f-0717-40dc-894b-fde58e6fed37",
   "metadata": {},
   "source": [
    "- **Image Classification:** We are given a **Training Set** of labeled images, asked to predict labels on **Test Set.** Common to report the **Accuracy** of predictions(fraction of correctly predicted images)\n",
    "\n",
    "- We introduced the **k-Nearest Neighbor Classifier**, which predicts the labels based on nearest images in the training set\n",
    "\n",
    "- We saw that the choice of distance and the value of k are **hyperparameters** that are tuned using a **validation set**, or through **cross-validation** if the size of the data is small.\n",
    "\n",
    "- Once the best set of hyperparameters is chosen, the classifier is evaluated once on the test set, and reported as the performance of kNN on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addedb3-7b0c-4d2e-ae66-5f0651f19789",
   "metadata": {},
   "source": [
    "- Nearest Neighbor 분류기는 CIFAR-10 데이터셋에서 약 40% 정도의 정확도를 보이는 것을 확인하였다. 이 방법은 구현이 매우 간단하지만, 학습 데이터셋 전체를 메모리에 저장해야 하고, 새로운 테스트 이미지를 분류하고 평가할 때 계산량이 매우 많다.\n",
    "\n",
    "- 단순히 픽셀 값들의 L1이나 L2 거리는 이미지의 클래스보다 배경이나 이미지의 전체적인 색깔 분포 등에 더 큰 영향을 받기 때문에 이미지 분류 문제에 있어서 충분하지 못하다는 점을 보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fda52a-1038-4b09-b774-0b2fa02fd758",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1345d-3cb9-46a5-a5d5-99772847f533",
   "metadata": {},
   "source": [
    "- We defined a **score function** from image pixels to class scores (in this section, a linear function that depends on weights **W** and biases **b**).\n",
    "\n",
    "- Unlike kNN classifier, the advantage of this **parametric approach** is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with **W**, not an exhaustive comparison to every single training example.\n",
    "\n",
    "- We introduced the **bias trick**, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix.\n",
    "하나의 매개변수 행렬만 추적해야 하는 편의를 위해 편향 벡터를 가중치 행렬로 접을 수 있는 편향 트릭을 도입했습니다 .\n",
    "\n",
    "- We defined a **loss function** (we introduced two commonly used losses for linear classifiers: the **SVM** and the **Softmax**) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b014d4-5b29-4818-9c1c-70a28eb57c83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e373643-a525-4ec7-9a36-67ad34783a9e",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf32a3-d8ea-469a-9c82-03d9f9626ba8",
   "metadata": {},
   "source": [
    "- We developed the intuition of the loss function as a **high-dimensional optimization landscape** in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped.\n",
    "\n",
    "- We motivated the idea of optimizing the loss function with **iterative refinement**, where we start with a random set of weights and refine them step by step until the loss is minimized.\n",
    "\n",
    "- We saw that the **gradient** of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of h used in computing the numerical gradient).\n",
    "\n",
    "- We saw that the parameter update requires a tricky setting of the **step size** (or the **learning rate**) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections.\n",
    "\n",
    "- We discussed the tradeoffs between computing the **numerical** and **analytic** gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a **gradient check**, in which its implementation is compared to the numerical gradient.\n",
    "\n",
    "- We introduced the **Gradient Descent** algorithm which iteratively computes the gradient and performs a parameter update in loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17e151-331e-48ed-be8c-2419d1af9235",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c5d41-2c75-4350-8595-a4477fb6a79d",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd54cf0-8ffd-4d60-b633-aff8d02ca8cf",
   "metadata": {},
   "source": [
    "- We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher.\n",
    "\n",
    "- We discussed the importance of **staged computation** for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5069b-9464-424e-a056-de2ab15483de",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Network part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709a149-7b03-4042-9865-f75422430fd1",
   "metadata": {},
   "source": [
    "- We introduced a very coarse model of a biological **neuron**\n",
    "\n",
    "- 실제 사용되는 몇몇 **활성화 함수** 에 대해 논의하였고, ReLU가 가장 일반적인 선택이다.\n",
    "\n",
    "- We introduced **Neural Networks** where neurons are connected with **Fully-Connected layers** where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.\n",
    "\n",
    "- 우리는 layered architecture를 통해 활성화 함수의 기능 적용과 결합된 행렬 곱을 기반으로 신경망을 매우 효율적으로 평가할 수 있음을 보았다.\n",
    "\n",
    "- 우리는 Neural Networks가 **universal function approximators** 임을 보았지만, 우리는 또한 이 특성이 그들의 편재적인 사용과 거의 관련이 없다는 사실에 대해 논의하였다. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice.\n",
    "\n",
    "- 우리는 큰 network가 작은 network보다 항상 더 잘 작동하지만, 더 높은 model capacity는 더 강력한 정규화(높은 가중치 감쇠같은)로 적절히 해결되어야 하며, 그렇지 않으면 오버핏될 수 있다는 사실에 대해 논의하였다. 이후 섹션에서 더 많은 형태의 정규화(특히 dropout)를 볼 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec8327-f167-456e-a9b9-e6764e380358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
